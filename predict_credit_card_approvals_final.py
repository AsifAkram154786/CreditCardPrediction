# -*- coding: utf-8 -*-
"""Predict_credit_card_approvals-Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nZ1DvMlO2FyYs0SJv8JcQxzYRlERfcFZ
"""

# Part 1: Setup and Data Loading

# Step 1: Install any needed libraries (if not already available)
!pip install -q pandas numpy scikit-learn matplotlib seaborn

# Step 2: Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix

# Step 3: Load the dataset
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data"
column_names = [
    "A1", "A2", "A3", "A4", "A5", "A6", "A7",
    "A8", "A9", "A10", "A11", "A12", "A13", "A14", "A15", "Class"
]

df = pd.read_csv(url, header=None, names=column_names)

# Step 4: Preview the data
print("Dataset shape:", df.shape)
df.head()

# Part 2: Exploratory Data Analysis (EDA) and Visualization

# Step 5: Summary statistics
print(df.describe(include='all'))

# Step 6: Missing value analysis
print("Missing values per column:\n", df.isnull().sum())

# Visualize missing values (heatmap)
plt.figure(figsize=(10, 6))
sns.heatmap(df.isnull(), cbar=False, yticklabels=False, cmap='viridis')
plt.title('Missing Value Heatmap')
plt.show()

# Step 7: Data type analysis
print("\nData types:\n", df.dtypes)

# Step 8: Explore categorical features
categorical_cols = df.select_dtypes(include=['object']).columns
for col in categorical_cols:
    print(f"\nValue counts for {col}:")
    print(df[col].value_counts())
    plt.figure(figsize=(8, 6))
    df[col].value_counts().plot(kind='bar')
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Count')
    plt.show()

# Step 9: Explore numerical features (if any) - Assuming A2, A3, A8, A11, A14, A15 are numerical after cleaning
numerical_cols = ['A2', 'A3', 'A8', 'A11', 'A14', 'A15'] # Replace with actual numerical columns after handling '?'

for col in numerical_cols:
    try:
        df[col] = pd.to_numeric(df[col], errors='coerce') # Convert to numeric, coerce errors to NaN
        plt.figure(figsize=(8, 6))
        sns.histplot(df[col].dropna(), kde=True) # Drop NaN for plotting, add kde for density estimation
        plt.title(f'Distribution of {col}')
        plt.xlabel(col)
        plt.ylabel('Frequency')
        plt.show()

        plt.figure(figsize=(8, 6))
        sns.boxplot(x=df[col])
        plt.title(f'Boxplot of {col}')
        plt.show()
    except (ValueError, TypeError) as e:
      print(f"Error processing {col}: {e}")
      print(f"Values: {df[col].unique()}")


# Step 10: Explore the relationship between features and the target variable ('Class')

# Example: Countplot of 'Class' vs. a categorical feature
for col in categorical_cols:
  plt.figure(figsize=(8,6))
  sns.countplot(x=col, hue='Class', data=df)
  plt.title(f"Class Distribution by {col}")
  plt.show()


# Example: Boxplot of numerical features vs. 'Class'
for col in numerical_cols:
  try:
    df[col] = pd.to_numeric(df[col], errors='coerce') # Convert to numeric, coerce errors to NaN
    plt.figure(figsize=(8, 6))
    sns.boxplot(x='Class', y=col, data=df)
    plt.title(f'Boxplot of {col} by Class')
    plt.show()
  except (ValueError, TypeError) as e:
    print(f"Error processing {col}: {e}")
    print(f"Values: {df[col].unique()}")

# Correlation Matrix (Heatmap) for numerical features after handling '?'
plt.figure(figsize=(12, 10))
correlation_matrix = df[numerical_cols].corr()  #only numerical values
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Numerical Features')
plt.show()

# Part 2: Data Preprocessing

# Step 1: Replace '?' with NaN
df.replace('?', np.nan, inplace=True)

# Step 2: Identify columns with missing values
missing_cols = df.columns[df.isnull().any()]
print("Columns with missing values:", missing_cols.tolist())

# Step 3: Handle missing values
# For numerical columns, fill with mean
for col in ['A2', 'A3', 'A8', 'A11', 'A14']:
    df[col] = pd.to_numeric(df[col], errors='coerce')  # convert to numeric
    df[col].fillna(df[col].mean(), inplace=True)

# For categorical columns, fill with mode
for col in df.columns:
    if df[col].dtype == 'object':
        df[col].fillna(df[col].mode()[0], inplace=True)

# Step 4: Encode categorical variables using Label Encoding
label_encoders = {}
for col in df.columns:
    if df[col].dtype == 'object':
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col])
        label_encoders[col] = le

# Step 5: Convert target column ('Class') to 0 and 1
# Assuming '+' means approved (1) and '-' means rejected (0)
# After encoding, this has already been done

# Step 6: Feature-target split
X = df.drop("Class", axis=1)
y = df["Class"]

# Step 7: Normalize numerical features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 8: Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

print("Training set size:", X_train.shape)
print("Test set size:", X_test.shape)

df.head()

print("Training set size:", X_train.shape)
print("Test set size:", X_test.shape)

from sklearn.linear_model import LogisticRegression

# Initialize and train the logistic regression model
logreg_model = LogisticRegression(random_state=42)
logreg_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = logreg_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred)

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")
print(f"ROC AUC: {roc_auc:.4f}")

print("\nClassification Report:\n", classification_report(y_test, y_pred))

print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

from sklearn.svm import SVC

# Initialize and train the SVM model
svm_model = SVC(random_state=42)  # You can adjust parameters like kernel, C, etc.
svm_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred_svm = svm_model.predict(X_test)

# Evaluate the SVM model
accuracy_svm = accuracy_score(y_test, y_pred_svm)
precision_svm = precision_score(y_test, y_pred_svm)
recall_svm = recall_score(y_test, y_pred_svm)
f1_svm = f1_score(y_test, y_pred_svm)
roc_auc_svm = roc_auc_score(y_test, y_pred_svm)

print(f"SVM Accuracy: {accuracy_svm:.4f}")
print(f"SVM Precision: {precision_svm:.4f}")
print(f"SVM Recall: {recall_svm:.4f}")
print(f"SVM F1-score: {f1_svm:.4f}")
print(f"SVM ROC AUC: {roc_auc_svm:.4f}")

print("\nSVM Classification Report:\n", classification_report(y_test, y_pred_svm))

print("\nSVM Confusion Matrix:\n", confusion_matrix(y_test, y_pred_svm))

# Create a DataFrame to store the results
results_df = pd.DataFrame({
    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC AUC'],
    'Logistic Regression': [accuracy, precision, recall, f1, roc_auc],
    'SVM': [accuracy_svm, precision_svm, recall_svm, f1_svm, roc_auc_svm]
})

# Display the results
print(results_df)

# Plotting the results (optional)
plt.figure(figsize=(10, 6))
sns.barplot(x='Metric', y='value', hue='Model', data=pd.melt(results_df, 'Metric', var_name='Model', value_name='value'))
plt.title('Model Comparison')
plt.ylabel('Score')
plt.show()

# Install XGBoost (if needed)
!pip install xgboost

# Import libraries
from xgboost import XGBClassifier

# Initialize and train the model
xgb_model = XGBClassifier(random_state=42)
xgb_model.fit(X_train, y_train)

# Make predictions
y_pred_xgb = xgb_model.predict(X_test)

# Evaluate the model
accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
precision_xgb = precision_score(y_test, y_pred_xgb)
recall_xgb = recall_score(y_test, y_pred_xgb)
f1_xgb = f1_score(y_test, y_pred_xgb)
roc_auc_xgb = roc_auc_score(y_test, y_pred_xgb)

print(f"XGBoost Accuracy: {accuracy_xgb:.4f}")
print(f"XGBoost Precision: {precision_xgb:.4f}")
print(f"XGBoost Recall: {recall_xgb:.4f}")
print(f"XGBoost F1-score: {f1_xgb:.4f}")
print(f"XGBoost ROC AUC: {roc_auc_xgb:.4f}")
print("\nXGBoost Classification Report:\n", classification_report(y_test, y_pred_xgb))
print("\nXGBoost Confusion Matrix:\n", confusion_matrix(y_test, y_pred_xgb))

# Update results DataFrame
results_df['XGBoost'] = [accuracy_xgb, precision_xgb, recall_xgb, f1_xgb, roc_auc_xgb]

# Plot updated results
plt.figure(figsize=(12, 6))
sns.barplot(
    x='Metric',
    y='value',
    hue='Model',
    data=pd.melt(results_df, 'Metric', var_name='Model', value_name='value')
)
plt.title('Model Comparison (Logistic Regression vs SVM vs XGBoost)')
plt.ylabel('Score')
plt.show()

# Plot feature importance
plt.figure(figsize=(10, 6))
feat_importances = pd.Series(xgb_model.feature_importances_, index=X.columns)
feat_importances.nlargest(10).plot(kind='barh')
plt.title('Top 10 Important Features (XGBoost)')
plt.show()

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Logistic Regression Feature Importance
plt.figure(figsize=(10, 6))
importance = logreg_model.coef_[0]  # Get coefficients
feat_importances = pd.Series(importance, index=X.columns)
feat_importances.nlargest(10).plot(kind='barh')
plt.title('Top 10 Important Features (Logistic Regression)')
plt.show()

# SVM Feature Importance (for linear kernel)
if svm_model.kernel == 'linear':  # Check if the kernel is linear
    plt.figure(figsize=(10, 6))
    importance = svm_model.coef_[0]  # Get feature weights
    feat_importances = pd.Series(np.abs(importance), index=X.columns) # Use absolute values
    feat_importances.nlargest(10).plot(kind='barh')
    plt.title('Top 10 Important Features (SVM - Linear Kernel)')
    plt.show()
else:
    print("Feature importance plotting for non-linear SVM kernels is not supported directly.")

import pandas as pd
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Define metrics for each model
models = {
    "Logistic Regression": y_pred,
    "SVM": y_pred_svm,
    "XGBoost": y_pred_xgb
}

metrics_table = []
for name, preds in models.items():
    metrics_table.append({
        "Model": name,
        "Accuracy": accuracy_score(y_test, preds),
        "Precision": precision_score(y_test, preds),
        "Recall": recall_score(y_test, preds),
        "F1-Score": f1_score(y_test, preds),
        "ROC AUC": roc_auc_score(y_test, preds)
    })

# Convert to DataFrame
metrics_df = pd.DataFrame(metrics_table)
print(metrics_df)

import shap

# Initialize SHAP explainer
# Use the trained XGBoost model 'xgb_model' instead of 'best_xgb'
explainer = shap.TreeExplainer(xgb_model)
shap_values = explainer.shap_values(X_test)

# Summary plot
shap.summary_plot(shap_values, X_test, feature_names=X.columns)

# Force plot for a specific prediction
shap.force_plot(explainer.expected_value, shap_values[0,:], X_test[0,:], feature_names=X.columns)

# SHAP for Logistic Regression
explainer_lr = shap.LinearExplainer(logreg_model, X_train)
shap_values_lr = explainer_lr.shap_values(X_test)

# Clip SHAP values to handle extreme negative and positive values
lower_threshold = np.percentile(shap_values_lr, 5)  # Clip values below 5th percentile
upper_threshold = np.percentile(shap_values_lr, 95) # Clip values above 95th percentile
shap_values_lr_clipped = np.clip(shap_values_lr, lower_threshold, upper_threshold)

shap.summary_plot(shap_values_lr_clipped, X_test, feature_names=X.columns, title="SHAP Summary Plot - Logistic Regression (Clipped)")